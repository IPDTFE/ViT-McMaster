{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Based on https://colab.research.google.com/drive/1Z1lbR_oTSaeodv9tTm11uEhOjhkUx1L4?usp=sharing"
      ],
      "metadata": {
        "id": "JKphRl6AFAAD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJrptgKkvtcC"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 1\n",
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE =  0.00002\n",
        "mixup = False\n",
        "alpha = 0.8\n",
        "gridsize = 1\n",
        "USESAM = False\n",
        "frozenlayers = 0\n",
        "imgname = \"ak064t1aaaff137\"\n",
        "explainability = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqBD5lIwbnTq"
      },
      "outputs": [],
      "source": [
        "!pip install ipython-autotime\n",
        "%load_ext autotime\n",
        "\n",
        "import typing\n",
        "import io\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "import csv\n",
        "import glob\n",
        "import sys\n",
        "import shutil\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import pathlib\n",
        "import random\n",
        "import statistics\n",
        "import statistics\n",
        "import torchvision\n",
        "import math\n",
        "import torch.utils.data as data\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "from torch.autograd import Variable\n",
        "from datetime import datetime\n",
        "from urllib.request import urlretrieve\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from torchvision.transforms import ToTensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQy0s2MHCM6z"
      },
      "outputs": [],
      "source": [
        "def rollout(attentions, discard_ratio, head_fusion):\n",
        "    result = torch.eye(attentions[0].size(-1))\n",
        "    with torch.no_grad():\n",
        "        for attention in attentions:\n",
        "            if head_fusion == \"mean\":\n",
        "                attention_heads_fused = attention.mean(axis=1)\n",
        "            elif head_fusion == \"max\":\n",
        "                attention_heads_fused = attention.max(axis=1)[0]\n",
        "            elif head_fusion == \"min\":\n",
        "                attention_heads_fused = attention.min(axis=1)[0]\n",
        "            else:\n",
        "                raise \"Attention head fusion type Not supported\"\n",
        "\n",
        "            # Drop the lowest attentions, but\n",
        "            # don't drop the class token\n",
        "            flat = attention_heads_fused.view(attention_heads_fused.size(0), -1)\n",
        "            _, indices = flat.topk(int(flat.size(-1)*discard_ratio), -1, False)\n",
        "            indices = indices[indices != 0]\n",
        "            flat[0, indices] = 0\n",
        "\n",
        "            I = torch.eye(attention_heads_fused.size(-1))\n",
        "            a = (attention_heads_fused + 1.0*I.to(device))/2\n",
        "            a = a / a.sum(dim=-1)\n",
        "\n",
        "            result = torch.matmul(a, result.to(device))\n",
        "    \n",
        "    # Look at the total attention between the class token,\n",
        "    # and the image patches\n",
        "    mask = result[0, 0 , 1 :]\n",
        "    # In case of 224x224 image, this brings us from 196 to 14\n",
        "    width = int(mask.size(-1)**0.5)\n",
        "    mask = mask.reshape(width, width).cpu().numpy()\n",
        "    mask = mask / np.max(mask)\n",
        "    return mask    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJ8AVUDhrQIf"
      },
      "outputs": [],
      "source": [
        "\n",
        "datasetpath = \"\"\n",
        "datasetpath2x2 = \"\"\n",
        "if gridsize == 1:\n",
        "  shutil.copy(datasetpath, \"/content/imagesFINAL2.zip\")\n",
        "elif gridsize == 2:\n",
        "  shutil.copy(datasetpath2x2, \"/content/imagesFINAL2.zip\")\n",
        "\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_aUIiX2rhqK"
      },
      "outputs": [],
      "source": [
        "shutil.unpack_archive(\"/content/imagesFINAL2.zip\", \"/content/imagesFINAL2\",\"zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__vkUQApWOWC"
      },
      "outputs": [],
      "source": [
        "lookup = {}\n",
        "pspifilepath = \"\"\n",
        "pspifolder = \"\"\n",
        "if not os.path.exists (pspifilepath):\n",
        "    for path, dirs, files in os.walk(pspifolder, topdown = True):\n",
        "        files = [ fi for fi in files if fi.endswith(\".txt\")]\n",
        "        for f in files:\n",
        "            txt = open(os.path.join(path, f), \"r\")\n",
        "            PSPI = int(float(txt.read()))\n",
        "            name = f[0:-9]\n",
        "            lookup[name] = PSPI\n",
        "\n",
        "    with open(pspifilepath, 'wb') as f:\n",
        "        pickle.dump([lookup], f)\n",
        "else:\n",
        "    with open(pspifilepath, 'rb') as f:\n",
        "        lookup = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELnD-iysa9DZ"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  del lookup[0][\"tv095t2aeunaff001\"]\n",
        "  del lookup[0][\"tv095t2aeunaff002\"]\n",
        "  del lookup[0][\"tv095t2aeunaff003\"]\n",
        "  del lookup[0][\"tv095t2aeunaff004\"]\n",
        "  del lookup[0][\"tv095t2aeunaff005\"]\n",
        "  del lookup[0][\"tv095t2aeunaff006\"]\n",
        "  del lookup[0][\"tv095t2aeunaff007\"]  \n",
        "except:\n",
        "  print(\"error\")\n",
        "\n",
        "if gridsize == 2:\n",
        "  try:\n",
        "    del lookup[0][\"tv095t2aeunaff008\"]\n",
        "    del lookup[0][\"tv095t2aeunaff009\"]\n",
        "    del lookup[0][\"tv095t2aeunaff010\"]\n",
        "  except:\n",
        "    print(\"error\")\n",
        "\n",
        "  kl = []\n",
        "  for key, value in lookup[0].items():\n",
        "    if \"001\" in key or  \"002\" in key or \"003\" in key:\n",
        "      kl += [key]\n",
        "      \n",
        "  for k in kl:\n",
        "    del lookup[0][k]\n",
        "\n",
        "with open('mycsvfile.csv','w') as f:\n",
        "    w = csv.writer(f)\n",
        "    w.writerows(lookup[0].items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TswbsNMgSxv2"
      },
      "outputs": [],
      "source": [
        "\n",
        "for root, dirs, files in os.walk(\"/content/imagesFINAL2\", topdown=True):\n",
        "    for name in files:\n",
        "       shutil.move(os.path.join(root,name), \"/content/imagesFINAL2/\" + name)\n",
        "\n",
        "for f in glob.glob('/content/imagesFINAL2/**', recursive=True):\n",
        "    if not f.endswith(\".png\") and os.path.isfile(f):\n",
        "        os.remove(f)\n",
        "\n",
        "\n",
        "root = '/content/imagesFINAL2'\n",
        "folders = list(os.walk(root, topdown= False))[0:]\n",
        "\n",
        "for folder in folders:\n",
        "    if not folder[2]:\n",
        "        try:\n",
        "            os.rmdir(folder[0])\n",
        "        except:\n",
        "            pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tvwvp2_8Ir4K",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "torch.use_deterministic_algorithms(True) \n",
        "def seed_everything(seed=42):\n",
        "  random.seed(seed)\n",
        "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything()\n",
        "labels_frame = pd.read_csv(\"/content/mycsvfile.csv\", header=None)\n",
        "la = [\"bn080\", \"mg101\", \"aa048\", \"jh043\", \"jh123\"]\n",
        "lb = [\"dn124\", \"vw121\", \"fn059\", \"dr052\", \"ll042\"]\n",
        "lc = [\"jk103\", \"jl047\", \"ch092\", \"jy115\", \"bg096\"]\n",
        "ld = [\"bm049\", \"kz120\", \"th108\", \"tv095\", \"ib109\"]\n",
        "le = [\"nm106\", \"mg066\", \"hs107\", \"gf097\", \"ak064\"]\n",
        "\n",
        "splits = [la,lb,lc,ld,le]\n",
        "if explainability == True:\n",
        "  splits = [la]\n",
        "avgf1 = []\n",
        "matrixlist = []\n",
        "\n",
        "presults = []\n",
        "\n",
        "\n",
        "for s in splits:\n",
        "  os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]=\":4096:8\"\n",
        "  try:\n",
        "    presults += [(poutput, plabels)]\n",
        "  except:\n",
        "    pass\n",
        "  poutput = []\n",
        "  plabels = []\n",
        "  class PainData(Dataset):\n",
        "    def __init__(self, mode):\n",
        "      self.labels_frame = pd.read_csv(\"/content/mycsvfile.csv\", header=None)\n",
        "      self.mode = mode\n",
        "      if mode == \"test\":\n",
        "        self.df = labels_frame[labels_frame[0].str.startswith(tuple(s))]\n",
        "      elif mode == \"train\":\n",
        "        self.df = labels_frame[labels_frame[0].str.startswith(tuple(s)) == False]\n",
        "      else:\n",
        "        sys.exit(\"mode needs to be 'train' or 'test'\")\n",
        "      self.transform = transforms.Compose([\n",
        "      # you can add other transformations in this list\n",
        "      transforms.ToTensor()\n",
        "      ])\n",
        "      self.ps = len(self.df[self.df[1] == 0])\n",
        "      self.ns = len(self.df) - self.ps\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.df.index)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      f = self.df.iloc[idx].to_numpy()\n",
        "      name = f[0]\n",
        "      label = torch.zeros(2)\n",
        "      if f[1] != 0:\n",
        "        label[1] = 1\n",
        "      else:\n",
        "        label[0] = 1\n",
        "\n",
        "      if self.mode == \"train\" and idx % 5 == 0 and idx > 0 and mixup == True and \"mg101\" not in f[0]:\n",
        "          # Choose another image/label randomly\n",
        "          mixup_idx = None\n",
        "          f2 = None  \n",
        "          while True:\n",
        "              mixup_idx = random.randrange(0, len(self.df.index))\n",
        "              f2 = self.df.iloc[mixup_idx].to_numpy()\n",
        "              f2e = f2[1]\n",
        "              fe = f[1]\n",
        "\n",
        "              if fe != 0:\n",
        "                fe = 1\n",
        "              \n",
        "              if f2e != 0:\n",
        "                f2e = 1\n",
        "              \n",
        "              if (fe != f2e and f[0][0:5] == f2[0][0:5]):\n",
        "                break\n",
        "          \n",
        "          name2 = f2[0]\n",
        "          label2 = f2[1]\n",
        "          mixup_label = torch.zeros(2)\n",
        "\n",
        "          if f2[1] != 0:\n",
        "            mixup_label[1] = 1\n",
        "          else:\n",
        "            mixup_label[0] = 1\n",
        "\n",
        "          if gridsize == 2:\n",
        "            image = self.transform(Image.open('/content/imagesFINAL2/' + name + \"_mesh.mat.png.png\"))\n",
        "            mixup_image = self.transform(Image.open('/content/imagesFINAL2/' + name2 + \"_mesh.mat.png.png\"))\n",
        "          else:\n",
        "            image = self.transform(Image.open('/content/imagesFINAL2/' + name + \"_mesh.mat.png\"))\n",
        "            mixup_image = self.transform(Image.open('/content/imagesFINAL2/' + name2 + \"_mesh.mat.png\"))\n",
        "          # Select a random number from the given beta distribution\n",
        "          # Mixup the images accordingly\n",
        "          lam = np.random.beta(alpha, alpha)\n",
        "          imagef = lam * image + (1 - lam) * mixup_image\n",
        "          labelf = lam * label + (1 - lam) * mixup_label\n",
        "          return imagef, labelf\n",
        "      if gridsize == 2:\n",
        "       return self.transform(Image.open('/content/imagesFINAL2/' + name + \"_mesh.mat.png.png\")), label \n",
        "      else:\n",
        "       return self.transform(Image.open('/content/imagesFINAL2/' + name + \"_mesh.mat.png\")), label \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  train_ds = PainData(\"train\")\n",
        "  test_ds = PainData(\"test\")\n",
        "\n",
        "\n",
        "  def resize_pos_embed(posemb, posemb_new): # example: 224:(14x14+1)-> 384: (24x24+1)\n",
        "      # Rescale the grid of position embeddings when loading from state_dict. Adapted from\n",
        "      # https://github.com/google-research/vision_transformer/blob/00883dd691c63a6830751563748663526e811cee/vit_jax/checkpoint.py#L224\n",
        "      ntok_new = posemb_new.shape[1]\n",
        "      if True:\n",
        "          posemb_tok, posemb_grid = posemb[:, :1], posemb[0, 1:]  # posemb_tok is for cls token, posemb_grid for the following tokens\n",
        "          ntok_new -= 1\n",
        "      else:\n",
        "          posemb_tok, posemb_grid = posemb[:, :0], posemb[0]\n",
        "      gs_old = int(math.sqrt(len(posemb_grid)))     # 14\n",
        "      gs_new = int(math.sqrt(ntok_new))             # 24\n",
        "      posemb_grid = posemb_grid.reshape(1, gs_old, gs_old, -1).permute(0, 3, 1, 2)  # [1, 196, dim]->[1, 14, 14, dim]->[1, dim, 14, 14]\n",
        "      posemb_grid = F.interpolate(posemb_grid, size=(gs_new, gs_new), mode='bicubic') # [1, dim, 14, 14] -> [1, dim, 24, 24]\n",
        "      posemb_grid = posemb_grid.permute(0, 2, 3, 1).reshape(1, gs_new * gs_new, -1)   # [1, dim, 24, 24] -> [1, 24*24, dim]\n",
        "      posemb = torch.cat([posemb_tok, posemb_grid], dim=1)   # [1, 24*24+1, dim]\n",
        "      return posemb\n",
        "\n",
        "  !pip install -q git+https://github.com/huggingface/transformers\n",
        "\n",
        "\n",
        "  from transformers import ViTModel\n",
        "  from transformers.modeling_outputs import SequenceClassifierOutput\n",
        "  from transformers import ViTConfig\n",
        "\n",
        "\n",
        "\n",
        "  model = ViTModel.from_pretrained('google/vit-base-patch16-224', output_attentions = True) # load pretrained model\n",
        "  state_dict = model.state_dict()\n",
        "\n",
        "\n",
        "  config = ViTConfig.from_pretrained('google/vit-base-patch16-224', image_size=224 * gridsize, output_attentions = True)\n",
        "  # new model with custom image_size\n",
        "  model = ViTModel(config=config)\n",
        "  # update state_dict\n",
        "  new_state_dict = state_dict.copy()\n",
        "  old_posemb = new_state_dict['embeddings.position_embeddings']\n",
        "\n",
        "  if gridsize == 2:\n",
        "    posemb_tok, posemb_grid = old_posemb[:, :1], old_posemb[0, 1:]  # posemb_tok is for cls token, posemb_grid for the following tokens\n",
        "    posemb_grid = posemb_grid.reshape(1, 14, 14, -1).permute(0, 3, 1, 2)  # [1, 196, dim]->[1, 14, 14, dim]->[1, dim, 14, 14]\n",
        "    posemb_grid2 = torch.cat((posemb_grid, posemb_grid), 3)\n",
        "    posemb_grid2 = torch.cat((posemb_grid2, posemb_grid2), 2)\n",
        "    posemb =  posemb_grid2.permute(0, 2, 3, 1).reshape(1, 28 * 28, -1)\n",
        "\n",
        "    posemb = torch.cat([posemb_tok, posemb], dim=1)   # [1, 24*24+1, dim]\n",
        "    old_posemb = posemb\n",
        "  new_state_dict['embeddings.position_embeddings'] = old_posemb\n",
        "\n",
        "  # equip new model with state_dict\n",
        "  model.load_state_dict(new_state_dict)\n",
        "\n",
        "  class ViTForImageClassification(nn.Module):\n",
        "      def __init__(self, num_labels=1):\n",
        "          super(ViTForImageClassification, self).__init__()\n",
        "          self.vit = model\n",
        "          self.dropout = nn.Dropout(0.1)\n",
        "          self.classifier = nn.Linear(self.vit.config.hidden_size, num_labels)\n",
        "          self.num_labels = num_labels\n",
        "\n",
        "      def forward(self, pixel_values, labels, att = False):\n",
        "          outputs = self.vit(pixel_values=pixel_values)\n",
        "          output = self.dropout(outputs.last_hidden_state[:,0])\n",
        "          logits = self.classifier(output)\n",
        "\n",
        "          loss = None\n",
        "          if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "          if loss is not None:\n",
        "            if att:\n",
        "              return logits, loss.item(), outputs.attentions\n",
        "            else:\n",
        "              return logits, loss.item()\n",
        "          else:\n",
        "             if att:\n",
        "              return logits, None, outputs.attentions\n",
        "             else:\n",
        "              return logits, None\n",
        "\n",
        "\n",
        "\n",
        "  class SAM(torch.optim.Optimizer):\n",
        "      def __init__(self, params, base_optimizer, rho=0.05, adaptive=False, **kwargs):\n",
        "          assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
        "\n",
        "          defaults = dict(rho=rho, adaptive=adaptive, **kwargs)\n",
        "          super(SAM, self).__init__(params, defaults)\n",
        "\n",
        "          self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
        "          self.param_groups = self.base_optimizer.param_groups\n",
        "          self.defaults.update(self.base_optimizer.defaults)\n",
        "\n",
        "      @torch.no_grad()\n",
        "      def first_step(self, zero_grad=False):\n",
        "          grad_norm = self._grad_norm()\n",
        "          for group in self.param_groups:\n",
        "              scale = group[\"rho\"] / (grad_norm + 1e-12)\n",
        "\n",
        "              for p in group[\"params\"]:\n",
        "                  if p.grad is None: continue\n",
        "                  self.state[p][\"old_p\"] = p.data.clone()\n",
        "                  e_w = (torch.pow(p, 2) if group[\"adaptive\"] else 1.0) * p.grad * scale.to(p)\n",
        "                  p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n",
        "\n",
        "          if zero_grad: self.zero_grad()\n",
        "\n",
        "      @torch.no_grad()\n",
        "      def second_step(self, zero_grad=False):\n",
        "          for group in self.param_groups:\n",
        "              for p in group[\"params\"]:\n",
        "                  if p.grad is None: continue\n",
        "                  p.data = self.state[p][\"old_p\"]  # get back to \"w\" from \"w + e(w)\"\n",
        "\n",
        "          self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n",
        "\n",
        "          if zero_grad: self.zero_grad()\n",
        "\n",
        "      @torch.no_grad()\n",
        "      def step(self, closure=None):\n",
        "          assert closure is not None, \"Sharpness Aware Minimization requires closure, but it was not provided\"\n",
        "          closure = torch.enable_grad()(closure)  # the closure should do a full forward-backward pass\n",
        "\n",
        "          self.first_step(zero_grad=True)\n",
        "          closure()\n",
        "          self.second_step()\n",
        "\n",
        "      def _grad_norm(self):\n",
        "          shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n",
        "          norm = torch.norm(\n",
        "                      torch.stack([\n",
        "                          ((torch.abs(p) if group[\"adaptive\"] else 1.0) * p.grad).norm(p=2).to(shared_device)\n",
        "                          for group in self.param_groups for p in group[\"params\"]\n",
        "                          if p.grad is not None\n",
        "                      ]),\n",
        "                      p=2\n",
        "                )\n",
        "          return norm\n",
        "\n",
        "      def load_state_dict(self, state_dict):\n",
        "          super().load_state_dict(state_dict)\n",
        "          self.base_optimizer.param_groups = self.param_groups\n",
        "\n",
        "  from transformers import ViTFeatureExtractor\n",
        "\n",
        "  # Define Model\n",
        "  model = ViTForImageClassification(2)    \n",
        "  # Feature Extractor\n",
        "  feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "  feature_extractor.size = 224 * gridsize\n",
        "  # Adam Optimizer\n",
        "  if USESAM == False:\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "  else:\n",
        "    base_optimizer = torch.optim.Adam\n",
        "    optimizer = SAM(model.parameters(), base_optimizer, lr=LEARNING_RATE)\n",
        "\n",
        "  loss_func = nn.CrossEntropyLoss()\n",
        "  # Use GPU if available  \n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
        "  if torch.cuda.is_available():\n",
        "      model.cuda() \n",
        "  \n",
        "  fln = []\n",
        "  for i in range(frozenlayers):\n",
        "    fln += [\"layer.\" + str(i) + \".\"]\n",
        "\n",
        "  for name, param in model.named_parameters():\n",
        "      a = False\n",
        "      param.requires_grad = True\n",
        "      if \"output\" in name:\n",
        "          param.requires_grad = False\n",
        "\n",
        "      for n in fln:\n",
        "        if n in name:\n",
        "            a = True\n",
        "      if a == True:\n",
        "        param.requires_grad = False\n",
        "  \n",
        "\n",
        "  ## Train the Model\n",
        "\n",
        "\n",
        "  nopain = train_ds.ns\n",
        "  pain = train_ds.ps\n",
        "  weights = []\n",
        "  for idx, t in enumerate(train_ds):\n",
        "    try:\n",
        "      if t[1][1] < 0.5:\n",
        "        weights.append(nopain)\n",
        "      else:\n",
        "        weights.append(pain)\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "\n",
        "  p = 0\n",
        "  sampler2 = data.WeightedRandomSampler(weights=weights, num_samples=len(train_ds), replacement=True)\n",
        "  train_loader = data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=False,  num_workers=2, sampler=sampler2)\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  # Train the model\n",
        "  for epoch in range(EPOCHS):        \n",
        "    for step, (x, y) in enumerate(train_loader):\n",
        "      # Change input array into list with each batch being one element\n",
        "      q = BATCH_SIZE\n",
        "      while True:\n",
        "          try:\n",
        "              x = np.split(np.squeeze(np.array(x)), len(x))\n",
        "              break\n",
        "          except:\n",
        "              q -= 1\n",
        "\n",
        "\n",
        "      # Remove unecessary dimension\n",
        "      for index, array in enumerate(x):\n",
        "        x[index] = np.squeeze(array)\n",
        "      # Apply feature extractor, stack back into 1 tensor and then convert to tensor\n",
        "      x = torch.tensor(np.stack(feature_extractor(x)['pixel_values'], axis=0))\n",
        "      # Send to GPU if available\n",
        "      x, y  = x.to(device), y.to(device)\n",
        "      b_x = Variable(x)   # batch x (image)\n",
        "      b_y = Variable(y)   # batch y (target)\n",
        "      # Feed through model\n",
        "      output, loss = model(b_x, None)\n",
        "\n",
        "      # Calculate loss\n",
        "      if loss is None:\n",
        "        if USESAM == True: \n",
        "          loss = loss_func(output, b_y)  # use this loss for any training statistics\n",
        "          loss.backward()\n",
        "          optimizer.first_step(zero_grad=True)\n",
        "          output, loss = model(b_x, None)\n",
        "\n",
        "          # second forward-backward pass\n",
        "          loss = loss_func(output, b_y) # make sure to do a full forward pass\n",
        "          loss.backward()\n",
        "          optimizer.second_step(zero_grad=True)\n",
        "\n",
        "        else:\n",
        "          loss = loss_func(output, b_y)   \n",
        "          optimizer.zero_grad()           \n",
        "          loss.backward()                 \n",
        "          optimizer.step()\n",
        "\n",
        "      if step % 10 == 0:\n",
        "        pass\n",
        "\n",
        "    state = {\n",
        "    'epoch': epoch,\n",
        "    'state_dict': model.state_dict(),\n",
        "    'optimizer': optimizer.state_dict(),\n",
        "    }\n",
        "\n",
        "    savepath = \"\"\n",
        "    torch.save(state, savepath + str(epoch) + '.pt')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  eval_loader  = data.DataLoader(test_ds, batch_size=1, shuffle=True, num_workers=2) \n",
        "    \n",
        "  yt = []\n",
        "  yp = []\n",
        "  for step, (x, y) in enumerate(eval_loader):\n",
        "    with torch.no_grad():\n",
        "\n",
        "      # Change input array into list with each batch being one element\n",
        "      q = BATCH_SIZE\n",
        "      while True:\n",
        "          try:\n",
        "              x = np.split(np.squeeze(np.array(x)), len(x))\n",
        "              break\n",
        "          except:\n",
        "              q -= 1\n",
        "\n",
        "      # Remove unecessary dimension\n",
        "      for index, array in enumerate(x):\n",
        "        x[index] = np.squeeze(array)\n",
        "      # Apply feature extractor, stack back into 1 tensor and then convert to tensor\n",
        "      x = torch.tensor(np.stack(feature_extractor(x)['pixel_values'], axis=0))\n",
        "      # Send to GPU if available\n",
        "      x, y  = x.to(device), y.to(device)\n",
        "      b_x = Variable(x)   # batch x (image)\n",
        "      b_y = Variable(y)   # batch y (target)\n",
        "      # Feed through model\n",
        "      output, _ = model(b_x, None)\n",
        "      poutput += [output]\n",
        "      plabels += [y]\n",
        "      predicted_class = output.argmax(1)\n",
        "\n",
        "      for n in y.tolist():\n",
        "        if n [0] == 1:\n",
        "          yt += [0]\n",
        "        else:\n",
        "          yt += [1]\n",
        "      yp += predicted_class.tolist()\n",
        "\n",
        "  cf_matrix = confusion_matrix(yt, yp)\n",
        "  matrixlist += [cf_matrix]\n",
        "  print(cf_matrix)\n",
        "  tn, fp, fn, tp = cf_matrix.ravel()\n",
        "  f1score = 2 * tp / (2*tp + fp + fn)\n",
        "  print(\"f1score: \", f1score)\n",
        "  avgf1 += [f1score]\n",
        "\n",
        "presults += [(poutput, plabels)]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22wjYXC2s6Ky"
      },
      "outputs": [],
      "source": [
        "if explainability == True:\n",
        "\n",
        "\n",
        "  eval_loader  = data.DataLoader(test_ds, batch_size=1, shuffle=True, num_workers=2)\n",
        "  model.eval()\n",
        "  th2 = 0.7\n",
        "  output_dir = ''\n",
        "  os.makedirs(output_dir, exist_ok=True)\n",
        "  convert_tensor = transforms.Compose([\n",
        "        transforms.ToTensor()\n",
        "        ])\n",
        "  if gridsize == 1:\n",
        "    x = convert_tensor(Image.open('/content/imagesFINAL2/' + imgname\t + \"_mesh.mat.png\")).unsqueeze(0)\n",
        "  if gridsize == 2:\n",
        "    x = convert_tensor(Image.open('/content/imagesFINAL2/' + imgname\t + \"_mesh.mat.png.png\")).unsqueeze(0)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    # Change input array into list with each batch being one element\n",
        "    q = BATCH_SIZE\n",
        "    while True:\n",
        "        try:\n",
        "            x = np.split(np.squeeze(np.array(x)), len(x))\n",
        "            break\n",
        "        except:\n",
        "            q -= 1\n",
        "\n",
        "    # Remove unecessary dimension\n",
        "    for index, array in enumerate(x):\n",
        "      x[index] = np.squeeze(array)\n",
        "    # Apply feature extractor, stack back into 1 tensor and then convert to tensor\n",
        "    x = torch.tensor(np.stack(feature_extractor(x)['pixel_values'], axis=0))\n",
        "    # Send to GPU if available\n",
        "    x, y  = x.to(device), y.to(device)\n",
        "    b_x = Variable(x)   # batch x (image)\n",
        "    b_y = Variable(y)   # batch y (target)\n",
        "    # Feed through model\n",
        "    outputs = model(b_x, None, True)\n",
        "    attentions = outputs[2][-1]\n",
        "    nh = 12 # number of head\n",
        "\n",
        "    # we keep only the output patch attention\n",
        "    attentions = attentions[0, :, 0, 1:].reshape(nh, -1)\n",
        "\n",
        "    threshold = 0\n",
        "    w_featmap = x.shape[-2] // 16\n",
        "    h_featmap = x.shape[-1] // 16\n",
        "\n",
        "    # we keep only a certain percentage of the mass\n",
        "    val, idx = torch.sort(attentions)\n",
        "    val /= torch.sum(val, dim=1, keepdim=True)\n",
        "    cumval = torch.cumsum(val, dim=1)\n",
        "    th_attn = cumval > (1 - threshold)\n",
        "    idx2 = torch.argsort(idx)\n",
        "    for head in range(nh):\n",
        "        th_attn[head] = th_attn[head][idx2[head]]\n",
        "    th_attn = th_attn.reshape(nh, w_featmap, h_featmap).float()\n",
        "    # interpolate\n",
        "    th_attn = nn.functional.interpolate(th_attn.unsqueeze(0), scale_factor=16, mode=\"nearest\")[0].cpu().numpy()\n",
        "\n",
        "    attentions = attentions.reshape(nh, w_featmap, h_featmap)\n",
        "    attentions = nn.functional.interpolate(attentions.unsqueeze(0), scale_factor=16, mode=\"nearest\")[0].cpu()\n",
        "    attentions = attentions.detach().numpy()\n",
        "\n",
        "    # show and save attentions heatmaps\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    torchvision.utils.save_image(torchvision.utils.make_grid(x, normalize=True, scale_each=True), os.path.join(output_dir, s[0] + \"gridsize\" + str(gridsize) + \"lr\" + str(LEARNING_RATE) + \"layers\" + str(12-frozenlayers) + \"img\" + \".png\"))\n",
        "\n",
        "    if gridsize == 1:\n",
        "      fff = np.zeros((224,224, 4))\n",
        "    if gridsize == 2: \n",
        "      fff = np.zeros((448,448, 4))\n",
        "\n",
        "    img = []\n",
        "\n",
        "    for j in range(nh):\n",
        "        fname = os.path.join(output_dir, s[0] + \"gridsize\" + str(gridsize) + \"lr\" + str(LEARNING_RATE) + \"layers\" + str(12-frozenlayers) + \"attn-head\" + str(j) + \".png\")\n",
        "        plt.figure()\n",
        "        plt.imsave(fname=fname, arr=attentions[j], format='png')\n",
        "        image_file = Image.open(fname)\n",
        "        image_file = image_file.point( lambda p: p if p > th2 * 255 else 0 )\n",
        "        fname = os.path.join(output_dir, \"thresholded\" + s[0] + \"gridsize\" + str(gridsize) + \"lr\" + str(LEARNING_RATE) + \"layers\" + str(12-frozenlayers) + \"attn-head\" + str(j) + \".png\")\n",
        "\n",
        "        image_file.save(fname)\n",
        "        if img == []:\n",
        "          img = np.array(image_file)\n",
        "        else:\n",
        "          img[0:3] = np.maximum(img, np.array(image_file))[0:3]\n",
        "\n",
        "        fff += image_file\n",
        "        \n",
        "    fff /= 12 * 255\n",
        "    plt.imshow(fff)\n",
        "    plt.colorbar()\n",
        "    plt.set_cmap('jet')\n",
        "    fname = os.path.join(output_dir, \"meanLL\" + s[0] + \"gridsize\" + str(gridsize) + \"lr\" + str(LEARNING_RATE) + \"layers\" + str(12-frozenlayers) + \"attn-head\" + str(j) + \".png\")\n",
        "    plt.imsave(fname=fname, arr=fff, format='png')\n",
        "    fname = os.path.join(output_dir, \"maxLL\" + s[0] + \"gridsize\" + str(gridsize) + \"lr\" + str(LEARNING_RATE) + \"layers\" + str(12-frozenlayers) + \"attn-head\" + str(j) + \".png\")\n",
        "    plt.imsave(fname=fname, arr=img, format='png')\n",
        "\n",
        "  if gridsize == 1:\n",
        "    x = convert_tensor(Image.open('/content/imagesFINAL2/' + imgname\t + \"_mesh.mat.png\")).unsqueeze(0)\n",
        "  if gridsize == 2:\n",
        "    x = convert_tensor(Image.open('/content/imagesFINAL2/' + imgname\t + \"_mesh.mat.png.png\")).unsqueeze(0)\n",
        "  with torch.no_grad():\n",
        "    if gridsize == 1:\n",
        "      x = F.interpolate(x, size=(224,224), mode='bicubic')\n",
        "    else:\n",
        "      x = F.interpolate(x, size=(448,448), mode='bicubic')\n",
        "    outputs = model(x.to(device), None, True)\n",
        "    attentions = outputs[2]\n",
        "    mask = rollout(attentions, 0.9, 'max')\n",
        "\n",
        "    fname = os.path.join(output_dir,  \"max\" + s[0] + \"gridsize\" + str(gridsize) + \"lr\" + str(LEARNING_RATE) + \"layers\" + str(12-frozenlayers) + \".png\")\n",
        "    if gridsize == 1:\n",
        "      res = cv2.resize(mask, dsize=(224, 224), interpolation=cv2.INTER_NEAREST)\n",
        "    else:\n",
        "      res = cv2.resize(mask, dsize=(448, 448), interpolation=cv2.INTER_NEAREST)\n",
        "    plt.imshow(res)\n",
        "    plt.colorbar()\n",
        "    plt.set_cmap('jet')\n",
        "    plt.imsave(fname=  fname, arr=res, format='png')\n",
        "    image_file = Image.open(fname)\n",
        "    image_file = image_file.point( lambda p: p if p > th2 * 255 else 0 )\n",
        "    fname = os.path.join(output_dir, \"thresholded\" + \"max\" + s[0] + \"gridsize\" + str(gridsize) + \"lr\" + str(LEARNING_RATE) + \"layers\" + str(12-frozenlayers) + \"attn-head\" + str(j) + \".png\")\n",
        "    image_file.save(fname)\n",
        "\n",
        "    mask = rollout(attentions, 0, 'mean')\n",
        "    fname = os.path.join(output_dir,  \"mean\" + s[0] + \"gridsize\" + str(gridsize) + \"lr\" + str(LEARNING_RATE) + \"layers\" + str(12-frozenlayers) + \".png\")\n",
        "    if gridsize == 1:\n",
        "      res = cv2.resize(mask, dsize=(224, 224), interpolation=cv2.INTER_NEAREST)\n",
        "    else:\n",
        "      res = cv2.resize(mask, dsize=(448, 448), interpolation=cv2.INTER_NEAREST)\n",
        "    plt.imshow(res)\n",
        "    plt.colorbar()\n",
        "    plt.set_cmap('jet')\n",
        "    plt.imsave(fname=  fname, arr=res, format='png')\n",
        "    image_file = Image.open(fname)\n",
        "    image_file = image_file.point( lambda p: p if p > th2 * 255 else 0 )\n",
        "    fname = os.path.join(output_dir, \"thresholded\" + \"mean\" + s[0] + \"gridsize\" + str(gridsize) + \"lr\" + str(LEARNING_RATE) + \"layers\" + str(12-frozenlayers) + \"attn-head\" + str(j) + \".png\")\n",
        "    image_file.save(fname)\n",
        "    fname = os.path.join(output_dir, s[0] + \"gridsize\" + str(gridsize) + \"lr\" + str(LEARNING_RATE) + \"layers\" + str(12-frozenlayers) + \".png\")\n",
        "    torchvision.utils.save_image(torchvision.utils.make_grid(x, normalize=True, scale_each=True), fname)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFb7rn-UqLWq"
      },
      "outputs": [],
      "source": [
        "\n",
        "e = []\n",
        "f = []\n",
        "tqtq = []\n",
        "\n",
        "for n in presults:\n",
        "  e = []\n",
        "  f = []\n",
        "  for a,b in zip(n[0], n[1]):\n",
        "    for c,d in zip(a,b):\n",
        "      c = c.cpu() \n",
        "      d = d.cpu()\n",
        "      softmax = np.exp(c[1])/sum(np.exp(c))\n",
        "      e += [softmax]\n",
        "      f += [d.cpu()[1]]\n",
        "  tqtq += [roc_auc_score(f, e)]\n",
        "  print(roc_auc_score(f, e))\n",
        "    \n",
        "    \n",
        "print(statistics.mean(tqtq))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlQg-mpkjGaZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "for m in matrixlist:\n",
        "  print(m, end =\"\\t\")\n",
        "print(\"\\nf1 scores \"  , avgf1)\n",
        "print(\"Mean f1 score \", statistics.mean(avgf1))\n",
        "print(\"STD f1 score \", statistics.stdev(avgf1))\n",
        "print(\"Layers used \", 12 - frozenlayers)\n",
        "print(\"Mixup \", mixup)\n",
        "if mixup == True:\n",
        "  print(\"Alpha \", alpha)\n",
        "print(\"learning rate \", LEARNING_RATE)\n",
        "print(\"Using SAM \", USESAM)\n",
        "print(\"Gridsize used \", gridsize)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}